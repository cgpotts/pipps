{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccdde30b-25c9-4b3c-88b8-2a19a2816d08",
   "metadata": {},
   "source": [
    "# PiPP wh-effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9979fbb8-6e49-4932-9133-c0e733df8512",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8638902-dd6b-48c9-aad9-20f241bc6c06",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0852ef59-4bd6-40b8-8d10-5291e925c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from minicons import scorer\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7155fd11-dd42-405d-a6c8-d4e7f5bb4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"pipp.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1b4f7-5d34-4be7-9c24-9d343368c328",
   "metadata": {},
   "source": [
    "## Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34afd0f3-b6c1-4478-910e-058267ef69d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PiPP (Filler/Gap)': ('Happy though we were with the idea, we had to reject it.',\n",
       "  'with'),\n",
       " 'PP (No Filler/No Gap)': ('Though we were happy with the idea, we had to reject it.',\n",
       "  'happy'),\n",
       " 'Filler/No Gap': ('Happy though we were happy with the idea, we had to reject it.',\n",
       "  'happy'),\n",
       " 'No Filler/Gap': ('Though we were with the idea, we had to reject it.',\n",
       "  'with')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.item(\"Happy* though we were GAP with the idea, we had to reject it.\", \n",
    "     embedding=\"\", \n",
    "     preposition=\"though\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8903d0a3-0e75-41ca-a743-3381a2d714ba",
   "metadata": {},
   "source": [
    "## Materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbdfeb52-d0ce-4a76-9f61-50f9a3bcd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"materials.txt\") as f:\n",
    "    materials = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a9f78e-2bef-4310-8f21-6affbdda532b",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28be26c7-5876-4615-a3b3-a1789a98598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_item(ex, item_num, model, embedding=\"\", preposition=\"though\"):\n",
    "    data = []\n",
    "    conds = utils.item(ex, embedding=embedding, preposition=preposition)\n",
    "    for typ, (text, target) in conds.items():\n",
    "        response = {}\n",
    "        response['fulltext'] = text\n",
    "        response['prompt'] = text\n",
    "        toks_with_logprobs = model.token_score([text], rank=False)[0]\n",
    "        toks, logprobs = zip(*toks_with_logprobs)\n",
    "        response['prompt_tokens'] = list(toks)\n",
    "        response['prompt_scores'] = list(logprobs)\n",
    "        inds = [i for i, tok in enumerate(toks) if tok.strip() == target]\n",
    "        if typ == 'Filler/No Gap':\n",
    "            # In this condition, there can be two identical tokens.\n",
    "            # This occurs when the PiPP is sentence-medial and so its\n",
    "            # nucleus phrase is not capitalized. The second token is \n",
    "            # always the one filling a GAP position.\n",
    "            ti = inds[-1]\n",
    "        else:\n",
    "            # In other conditions, where there is a gap, the token\n",
    "            # right after the hypothesized gap spot is sometimes\n",
    "            # incidentally repeated later in the example, but never\n",
    "            # before, so we can use the first.\n",
    "            ti = inds[0]\n",
    "        surprisal = convert_to_surprisal(logprobs[ti])\n",
    "        response['condition'] = typ\n",
    "        response['target_surprisal'] = surprisal\n",
    "        response['item_num'] = item_num\n",
    "        data.append(response)\n",
    "    return data\n",
    "\n",
    "def convert_to_surprisal(x):\n",
    "    return -(x / np.log(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b49243e2-9a5e-415b-8e66-d012dee98db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(materials, model_name, embedding=\"\", preposition=\"though\"):\n",
    "    data = []\n",
    "    model = scorer.IncrementalLMScorer(model_name)\n",
    "    for item_num, m in enumerate(materials, start=1):\n",
    "        data += run_item(m, item_num, model, embedding=embedding, preposition=preposition)\n",
    "    emb = f\"-{embedding.replace(' ', '_')}\" if embedding else \"\"\n",
    "    model_nickname = model_name.split(\"/\")[-1]\n",
    "    output_filename = f\"results/results-{model_nickname}-{preposition}{emb}.json\"\n",
    "    with open(output_filename, \"wt\") as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e45c6ea-9d2a-4a8a-9383-96cd00f67162",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69c0659a-e872-4029-9494-0ad0f13d906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_names = [\n",
    "    \"EleutherAI/pythia-70m-deduped\",\n",
    "    \"EleutherAI/pythia-160m-deduped\",\n",
    "    \"EleutherAI/pythia-410m-deduped\",\n",
    "    \"EleutherAI/pythia-1b-deduped\",\n",
    "    \"EleutherAI/pythia-1.4b-deduped\",\n",
    "    \"EleutherAI/pythia-2.8b-deduped\",\n",
    "    \"EleutherAI/pythia-6.9b-deduped\"\n",
    "    \"EleutherAI/pythia-12b-deduped\"\n",
    "]\n",
    "\n",
    "for model_name in all_model_names:\n",
    "    for embedding in (\"\", \"they said that we knew that\"):\n",
    "        for prep in (\"though\", \"as\", \"asas\"):\n",
    "            run_experiment(materials, model_name, embedding=embedding, preposition=prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219a605-ce4e-4399-a151-917ecdeb0c31",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b58b10f-daad-4c2c-957e-38ad3d0ef3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['PiPP (Filler/Gap)',  'No Filler/Gap', 'Filler/No Gap', 'PP (No Filler/No Gap)'][::-1]\n",
    "\n",
    "for results_filename in glob.glob(os.path.join(\"results\", \"results-pythia*.json\")):\n",
    "    utils.mean_plot(results_filename, order, xlim=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
